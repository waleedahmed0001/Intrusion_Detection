# -*- coding: utf-8 -*-
"""final_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WnWYXxGxKAWKoQQ7eC_IwVx-VMmi2EF
"""

cols="""duration,
protocol_type,
service,
flag,
src_bytes,
dst_bytes,
land,
wrong_fragment,
urgent,
hot,
num_failed_logins,
logged_in,
num_compromised,
root_shell,
su_attempted,
num_root,
num_file_creations,
num_shells,
num_access_files,
num_outbound_cmds,
is_host_login,
is_guest_login,
count,
srv_count,
serror_rate,
srv_serror_rate,
rerror_rate,
srv_rerror_rate,
same_srv_rate,
diff_srv_rate,
srv_diff_host_rate,
dst_host_count,
dst_host_srv_count,
dst_host_same_srv_rate,
dst_host_diff_srv_rate,
dst_host_same_src_port_rate,
dst_host_srv_diff_host_rate,
dst_host_serror_rate,
dst_host_srv_serror_rate,
dst_host_rerror_rate,
dst_host_srv_rerror_rate"""

columns=[]
for c in cols.split(','):
    if(c.strip()):
       columns.append(c.strip())

columns.append('outcome')

attacks_types = {
    'normal': 'normal',
'back': 'dos',
'buffer_overflow': 'u2r',
'ftp_write': 'r2l',
'guess_passwd': 'r2l',
'imap': 'r2l',
'ipsweep': 'probe',
'land': 'dos',
'loadmodule': 'u2r',
'multihop': 'r2l',
'neptune': 'dos',
'nmap': 'probe',
'perl': 'u2r',
'phf': 'r2l',
'pod': 'dos',
'portsweep': 'probe',
'rootkit': 'u2r',
'satan': 'probe',
'smurf': 'dos',
'spy': 'r2l',
'teardrop': 'dos',
'warezclient': 'r2l',
'warezmaster': 'r2l',
}

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
path='/content/drive/MyDrive/kddcup.csv.data'
df = pd.read_csv(path, names=columns)
df['outcome'] = df.outcome.apply(lambda r:attacks_types[r[:-1]])
df.head()

df.shape

print("Read {} rows.".format(len(df)))
print('='*40)
print('The number of data points are:', df.shape[0])
print('='*40)
print('The number of features are:', df.shape[1])
print('='*40)
output = df['outcome'].values
labels = set(output)
print('The different type of output labels are:', labels)
print('='*125)
print('The number of different output labels are:', len(labels))

# Checking for NULL values
print('Null values in dataset are',len(df[df.isnull().any(1)]))
print('='*40)

# Checkng for DUPLICATE values
df.drop_duplicates(keep='first', inplace = True)

# For now, just drop NA's (rows with missing values)
df.dropna(inplace=True,axis=1) 

# stored the data into a pickle file so we can load through
# df.to_pickle('df.pkl')

print("Read {} rows.".format(len(df)))

# Exploratory data analysis
import matplotlib.pyplot as plt
from matplotlib.pyplot import *

plt.figure(figsize=(15,7))
class_distribution = df['outcome'].value_counts()
class_distribution.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Data points per Class')
plt.title('Distribution of yi in train data')
plt.grid()
plt.show()

class_distribution

def mappData(column):
  lable=0
  resultDict={}
  for item in df[column].unique():
    df[column]=df[column].replace([item],[lable])
    resultDict[lable]=item
    lable+=1
  return resultDict

protocol_type_map=mappData("protocol_type")
print(protocol_type_map)

service_map=mappData("service")
print(service_map)

flag_map=mappData("flag")
print(flag_map)

X=df.drop(['outcome'],axis=1)

from sklearn.preprocessing import StandardScaler
Scaler= StandardScaler()
for col in X:
  # data[col].fillna(data[col].mean(),inplace=True)
  X[col]=Scaler.fit_transform(X[col].values.reshape(-1,1))

dummies = pd.get_dummies(df['outcome']) # Classification
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values

dummies

df.groupby('outcome')['outcome'].count()

import pandas as pd
import io
import requests
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn import metrics
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.callbacks import EarlyStopping

# Create a test/train split.  25% test
# Split into train/test
x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42,stratify=y)

# Create neural net
model = Sequential()
model.add(Dense(X.shape[1], input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
model.add(Dense(70, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
model.add(Dense(35, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))
model.add(Dense(y.shape[1],activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')
model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=10)

import tensorflow.keras.backend as K
print('Learning Rate - ')
print(K.eval(model.optimizer.lr)) 
print('='*50)
model.summary()

import seaborn as sns
import datetime as dt

def confusion_matrix_func(y_test, y_test_pred):
    
    '''
    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap
    '''
    C = confusion_matrix(y_test, y_test_pred)
    cm_df = pd.DataFrame(C)
    labels = ['dos', 'normal', 'probe', 'r2l', 'u2r']
    plt.figure(figsize=(20,15))
    sns.set(font_scale=1.4)
    sns.heatmap(cm_df, annot=True, annot_kws={"size":12}, fmt='g', xticklabels=labels, yticklabels=labels)
    plt.ylabel('Actual Class')
    plt.xlabel('Predicted Class')
    
    plt.show()

# calculate roc curve
from sklearn.metrics import *
#fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, pred)
from sklearn import preprocessing
def multiclass_roc_auc_score(y_test, pred, average="macro"):
    lb = preprocessing.LabelBinarizer()
    lb.fit(y_test)
    y_test = lb.transform(y_test)
    pred = lb.transform(pred)
    return roc_auc_score(y_test, pred, average=average)

print('Train data')
print(x_train.shape)
print(y_train.shape)
print('='*20)
print('Test data')
print(x_test.shape)
print(y_test.shape)
print('='*20)

# Measure accuracy
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
print('Predicting on the test data:')
start = dt.datetime.now()
escore = model.evaluate(x_test, y_test, batch_size=32)
pred = model.predict(x_test)
pred = np.argmax(pred,axis=1)
y_eval = np.argmax(y_test,axis=1)

vscore = metrics.accuracy_score(y_eval, pred)

rscore = recall_score(y_eval, pred, average='weighted')

ascore = precision_score(y_eval, pred, average='weighted')

f1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) / (precision + recall) for manual

roc_auc_socre = multiclass_roc_auc_score(y_eval, pred)



print('Completed')
print('Time taken:',dt.datetime.now()-start)
print('='*50)
print("Validation score: {}".format(vscore))
print('='*50)
print("Evaluation score: {}".format(escore))
print('='*50)
print("Recall score: {}".format(rscore))
print('='*50)
print("Precision score: {}".format(ascore))
print('='*50)
print("F1 score: {}".format(f1score))
print('='*50)
print("ROC-AUC score: {}".format(roc_auc_socre))

import pickle
pickle.dump(model, open('RnnModel.pkl', 'wb'))

confusion_matrix_func(y_eval, pred)



pred

y_test

get=model.predict(x_test)

get[0]

print(pred.unique(),pred.nunique())

print(np.unique(pred))

outcome={1: 'normal', 4: 'u2r', 0: 'dos', 3: 'r2l', 2: 'probe'}

final_answer=list(outcome.values())[list(outcome.keys()).index(pred[1])]

final_answer

f=outcome[pred[0]]
f

def preprocessing(value):
  st = value.split(",")
  for i in range(len(st)): 
    # print(x[i]) 
    st[i]=st[i].strip()
  print(type(st))

  return st

x=preprocessing("0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,511,511,0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00")

x[1]=list(protocol_type_map.keys())[list(protocol_type_map.values()).index(x[1])]

x[2]=list(service_map.keys())[list(service_map.values()).index(x[2])]

x[3]=list(flag_map.keys())[list(flag_map.values()).index(x[3])]

data=df.drop(['outcome'],axis=1)

df_length = len(data)
data.loc[df_length] = x

from sklearn.preprocessing import StandardScaler
Scaler= StandardScaler()
for col in data:
  # data[col].fillna(data[col].mean(),inplace=True)
  data[col]=Scaler.fit_transform(data[col].values.reshape(-1,1))

last_column = data.tail(1)

ans=model.predict(last_column)

ans

ans = np.argmax(ans,axis=1)

f=outcome[ans[0]]

f

import tensorflow
# localhost_save_option = tensorflow.saved_model.SaveOptions(experimental_io_device="/job:localhost")
# model.save('/content/drive/MyDrive/RNNFOLDER', options=localhost_save_option)

model2 = tensorflow.keras.models.load_model('/content/drive/MyDrive/RNNFOLDER')

model2.summary()

get=model2.predict(x_test)

get

import pandas as pd
  
# import numpy as np
import numpy as np
  
# simple array
data = np.array([99, 82])
  
# providing an index
ser = pd.Series(data, index =['MLP', 'Sequential'])
print(ser)

# # Exploratory data analysis
# import matplotlib.pyplot as plt
# from matplotlib.pyplot import *

# plt.figure(figsize=(10,7))
# # class_distribution = df['outcome'].value_counts()
# ser.plot(kind='bar')
# plt.xlabel('Model')
# plt.ylabel('Accuracy')
# plt.title('Model Accuracy Graph')
# plt.grid()
# plt.show()

history=model2

loss_train = history.history['acc']
loss_val = history.history['val_acc']
epochs = range(1,10)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

